{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e0c11d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19cd54d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install geemap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd13e93f",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e87f7b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from matplotlib import pyplot\n",
    "from io import StringIO\n",
    "\n",
    "#ee.Authenticate()  # Only needed for the first time\n",
    "ee.Initialize()\n",
    "Map= geemap.Map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a06fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOC DATA \n",
    "data_doc = pd.read_excel('LTM_Data_2023_3_9.xlsx', usecols=['DOC_MG_L','SITE_ID', 'DATE_SMP'])\n",
    "\n",
    "# Read the \"Site Information\" Excel sheet to get the mapping between lake names and SITE_ID\n",
    "site_info_df = pd.read_excel('Site_Information_2022_8_1 (2).xlsx', usecols=['SITE_ID', 'PROGRAM_ID', 'LAKE_DEPTH_MEAN', 'SITE_NAME'])\n",
    "\n",
    "\n",
    "# PROF. Coordinates \n",
    "data_path = 'ALTM-50-stations.xlsx'\n",
    "df_lake_info = pd.read_excel(data_path, sheet_name='updated station coordinates', usecols=['SITE_ID', 'SITE_NAME','LATDD_CENT', 'LONDD_CENT'])\n",
    "\n",
    "\n",
    "merge_data = pd.merge(data_doc, site_info_df, on='SITE_ID')\n",
    "merge_data = pd.merge(merge_data, df_lake_info, on= ['SITE_ID', 'SITE_NAME'])\n",
    "\n",
    "data_modify = merge_data[merge_data['PROGRAM_ID']=='LTM_ALTM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b874009f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SITE_ID</th>\n",
       "      <th>DATE_SMP</th>\n",
       "      <th>DOC_MG_L</th>\n",
       "      <th>PROGRAM_ID</th>\n",
       "      <th>SITE_NAME</th>\n",
       "      <th>LAKE_DEPTH_MEAN</th>\n",
       "      <th>LATDD_CENT</th>\n",
       "      <th>LONDD_CENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>040850</td>\n",
       "      <td>1997-09-22</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>LTM_ALTM</td>\n",
       "      <td>Squaw Lake</td>\n",
       "      <td>3.4</td>\n",
       "      <td>43.63276</td>\n",
       "      <td>-74.73863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>040850</td>\n",
       "      <td>1995-03-02</td>\n",
       "      <td>3.031000</td>\n",
       "      <td>LTM_ALTM</td>\n",
       "      <td>Squaw Lake</td>\n",
       "      <td>3.4</td>\n",
       "      <td>43.63276</td>\n",
       "      <td>-74.73863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>040850</td>\n",
       "      <td>1996-08-19</td>\n",
       "      <td>3.595000</td>\n",
       "      <td>LTM_ALTM</td>\n",
       "      <td>Squaw Lake</td>\n",
       "      <td>3.4</td>\n",
       "      <td>43.63276</td>\n",
       "      <td>-74.73863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>040850</td>\n",
       "      <td>2003-11-06</td>\n",
       "      <td>4.232000</td>\n",
       "      <td>LTM_ALTM</td>\n",
       "      <td>Squaw Lake</td>\n",
       "      <td>3.4</td>\n",
       "      <td>43.63276</td>\n",
       "      <td>-74.73863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>040850</td>\n",
       "      <td>2005-07-11</td>\n",
       "      <td>3.344000</td>\n",
       "      <td>LTM_ALTM</td>\n",
       "      <td>Squaw Lake</td>\n",
       "      <td>3.4</td>\n",
       "      <td>43.63276</td>\n",
       "      <td>-74.73863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17863</th>\n",
       "      <td>1A3-048</td>\n",
       "      <td>2011-05-18</td>\n",
       "      <td>4.283944</td>\n",
       "      <td>LTM_ALTM</td>\n",
       "      <td>Grass Pond</td>\n",
       "      <td>1.5</td>\n",
       "      <td>43.69207</td>\n",
       "      <td>-75.06172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17864</th>\n",
       "      <td>1A3-048</td>\n",
       "      <td>2015-06-10</td>\n",
       "      <td>5.509394</td>\n",
       "      <td>LTM_ALTM</td>\n",
       "      <td>Grass Pond</td>\n",
       "      <td>1.5</td>\n",
       "      <td>43.69207</td>\n",
       "      <td>-75.06172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17865</th>\n",
       "      <td>1A3-048</td>\n",
       "      <td>2016-07-14</td>\n",
       "      <td>4.884900</td>\n",
       "      <td>LTM_ALTM</td>\n",
       "      <td>Grass Pond</td>\n",
       "      <td>1.5</td>\n",
       "      <td>43.69207</td>\n",
       "      <td>-75.06172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17866</th>\n",
       "      <td>1A3-048</td>\n",
       "      <td>2016-09-15</td>\n",
       "      <td>6.344000</td>\n",
       "      <td>LTM_ALTM</td>\n",
       "      <td>Grass Pond</td>\n",
       "      <td>1.5</td>\n",
       "      <td>43.69207</td>\n",
       "      <td>-75.06172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17867</th>\n",
       "      <td>1A3-048</td>\n",
       "      <td>2017-05-10</td>\n",
       "      <td>4.644200</td>\n",
       "      <td>LTM_ALTM</td>\n",
       "      <td>Grass Pond</td>\n",
       "      <td>1.5</td>\n",
       "      <td>43.69207</td>\n",
       "      <td>-75.06172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15821 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SITE_ID   DATE_SMP  DOC_MG_L PROGRAM_ID   SITE_NAME  LAKE_DEPTH_MEAN  \\\n",
       "0       040850 1997-09-22  3.100000   LTM_ALTM  Squaw Lake              3.4   \n",
       "2       040850 1995-03-02  3.031000   LTM_ALTM  Squaw Lake              3.4   \n",
       "4       040850 1996-08-19  3.595000   LTM_ALTM  Squaw Lake              3.4   \n",
       "6       040850 2003-11-06  4.232000   LTM_ALTM  Squaw Lake              3.4   \n",
       "8       040850 2005-07-11  3.344000   LTM_ALTM  Squaw Lake              3.4   \n",
       "...        ...        ...       ...        ...         ...              ...   \n",
       "17863  1A3-048 2011-05-18  4.283944   LTM_ALTM  Grass Pond              1.5   \n",
       "17864  1A3-048 2015-06-10  5.509394   LTM_ALTM  Grass Pond              1.5   \n",
       "17865  1A3-048 2016-07-14  4.884900   LTM_ALTM  Grass Pond              1.5   \n",
       "17866  1A3-048 2016-09-15  6.344000   LTM_ALTM  Grass Pond              1.5   \n",
       "17867  1A3-048 2017-05-10  4.644200   LTM_ALTM  Grass Pond              1.5   \n",
       "\n",
       "       LATDD_CENT  LONDD_CENT  \n",
       "0        43.63276   -74.73863  \n",
       "2        43.63276   -74.73863  \n",
       "4        43.63276   -74.73863  \n",
       "6        43.63276   -74.73863  \n",
       "8        43.63276   -74.73863  \n",
       "...           ...         ...  \n",
       "17863    43.69207   -75.06172  \n",
       "17864    43.69207   -75.06172  \n",
       "17865    43.69207   -75.06172  \n",
       "17866    43.69207   -75.06172  \n",
       "17867    43.69207   -75.06172  \n",
       "\n",
       "[15821 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2d50a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    " s2_new = ee.ImageCollection('COPERNICUS/S2_SR') \\\n",
    "        .filter(ee.Filter.calendarRange(2019, 2023, 'year')) \\\n",
    "        .filterBounds(lake) \\\n",
    "        .filter(ee.Filter.lt('CLOUD_COVER', 15)) \\\n",
    "        .map(maskS2clouds) \\\n",
    "        .map(normalizedBands)\\\n",
    "        .select(new_bands,STD_NEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcbb592a",
   "metadata": {},
   "outputs": [
    {
     "ename": "EEException",
     "evalue": "Image.bandNames: Parameter 'image' is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ee/data.py:345\u001b[0m, in \u001b[0;36m_execute_cloud_call\u001b[0;34m(call, num_retries)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 345\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_retries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m googleapiclient\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mHttpError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/googleapiclient/_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/googleapiclient/http.py:938\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpError(resp, content, uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muri)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostproc(resp, content)\n",
      "\u001b[0;31mHttpError\u001b[0m: <HttpError 400 when requesting https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/value:compute?prettyPrint=false&alt=json returned \"Image.bandNames: Parameter 'image' is required.\". Details: \"Image.bandNames: Parameter 'image' is required.\">",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEEException\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m s2_image \u001b[38;5;241m=\u001b[39m s2_new\u001b[38;5;241m.\u001b[39mfirst()  \u001b[38;5;66;03m# Get the first image from the s2_new collection\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m band_names \u001b[38;5;241m=\u001b[39m \u001b[43ms2_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbandNames\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetInfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Get the band names of the image\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(band_names)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ee/computedobject.py:96\u001b[0m, in \u001b[0;36mComputedObject.getInfo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetInfo\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     91\u001b[0m   \u001b[38;5;124;03m\"\"\"Fetch and return information about this object.\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m  Returns:\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m    The object can evaluate to anything.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomputeValue\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ee/data.py:954\u001b[0m, in \u001b[0;36mcomputeValue\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    951\u001b[0m body \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpression\u001b[39m\u001b[38;5;124m'\u001b[39m: serializer\u001b[38;5;241m.\u001b[39mencode(obj, for_cloud_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)}\n\u001b[1;32m    952\u001b[0m _maybe_populate_workload_tag(body)\n\u001b[0;32m--> 954\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_execute_cloud_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_get_cloud_projects\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_get_projects_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprettyPrint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ee/data.py:347\u001b[0m, in \u001b[0;36m_execute_cloud_call\u001b[0;34m(call, num_retries)\u001b[0m\n\u001b[1;32m    345\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m call\u001b[38;5;241m.\u001b[39mexecute(num_retries\u001b[38;5;241m=\u001b[39mnum_retries)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m googleapiclient\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mHttpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 347\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m _translate_cloud_exception(e)\n",
      "\u001b[0;31mEEException\u001b[0m: Image.bandNames: Parameter 'image' is required."
     ]
    }
   ],
   "source": [
    "s2_image = s2_new.first()  # Get the first image from the s2_new collection\n",
    "band_names = s2_image.bandNames().getInfo()  # Get the band names of the image\n",
    "print(band_names)  # Print the band names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959d227c",
   "metadata": {},
   "source": [
    "# REFLECTANCE data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e3f617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from matplotlib import pyplot\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "Map= geemap.Map()\n",
    "\n",
    "# Authenticate and initialize the Earth Engine API\n",
    "#ee.Authenticate()  # Only needed for the first time\n",
    "ee.Initialize()\n",
    "\n",
    "# Create a list of lake names\n",
    "lake_names = [\n",
    "    'Little Hope Pond', 'Big Hope Pond', 'East Copperas Pond', 'Sunday Pond', 'Sochia Pond',\n",
    "    'Grass Pond (3)', 'Little Clear Pond', 'Loon Hollow Pond', 'Willys Lake', 'Woods Lake',\n",
    "    'Middle Settlement Lake', 'Middle Branch Lake', 'Limekiln Lake', 'Squaw Lake', 'Indian Lake',\n",
    "    'Brook Trout Lake', 'Lost Pond', 'North Lake', 'Willis Lake', 'Long Pond', 'Carry Pond',\n",
    "    'Lake Colden', 'Avalanche Lake', 'Little Simon Pond', 'Raquette Lake Reservoir', 'G Lake',\n",
    "    'Constable Pond', 'Middle Pond', 'Arbutus Pond', 'Sagamore Lake', 'Black Pond', 'Windfall Pond',\n",
    "    'Queer Lake', 'Heart Lake', 'Big Moose Lake', 'Cascade Lake', 'Dart Lake', 'Little Echo Pond',\n",
    "    'Moss Lake', 'Lake Rondaxe', 'Squash Pond', 'West Pond', 'Owen Pond', 'Jockeybush Lake',\n",
    "    'Barnes Lake', 'Clear Pond', 'Otter Lake', 'Nate Pond', 'Grass Pond', 'South Lake (East Branch)'\n",
    "]\n",
    "\n",
    "# Define the Landsat 5 bands and their corresponding standard names\n",
    "# LC5_BANDS = ['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7', 'ST_B6', 'QA_PIXEL']\n",
    "# STD_NAMES = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2', 'temp', 'QA']\n",
    "\n",
    "sentinel2_bands =['B1','B2','B3','B4','B5','B6','QA60','B8A','B11']\n",
    "STD_NAMES = ['Aerosols', 'Blue', 'Green', 'Red', 'RedEdge1','RedEdge2','Cloud','NIR','SWIR1']\n",
    "\n",
    "\n",
    "def normalizedBands(image):\n",
    "    # Calculate the differences between NIR and other bands\n",
    "    nir = image.select('B8A').rename('NIR')\n",
    "    blue_nir_diff = nir.subtract(image.select('B2')).rename('blue_nir_diff')\n",
    "    green_nir_diff = nir.subtract(image.select('B3')).rename('green_nir_diff')\n",
    "    red_nir_diff = nir.subtract(image.select('B4')).rename('red_nir_diff')\n",
    "    lake_label = nir.multiply(0).add(34).rename('LAKE_LABEL')\n",
    "\n",
    "    return image.addBands(blue_nir_diff, None, True).addBands(green_nir_diff, None, True).addBands(red_nir_diff, None, True).addBands(lake_label, None, True)\n",
    "\n",
    "new_bands= ['blue_nir_diff','green_nir_diff','red_nir_diff','NIR','LAKE_LABEL','B5','B6']\n",
    "STD_NEW= ['blue_nir_diff','green_nir_diff','red_nir_diff','NIR','LAKE_LABEL','RedEdge1','RedEdge2']\n",
    "\n",
    "\n",
    "# Define the function to mask clouds and cloud shadows from Landsat 5 surface reflectance imagery\n",
    "def maskS2clouds(image):\n",
    "    qa = image.select('QA60')\n",
    "    # Bits 10 and 11 are clouds and cirrus, respectively.\n",
    "    cloudBitMask = 1 << 10\n",
    "    cirrusBitMask = 1 << 11\n",
    "\n",
    "    # Create the cloud mask\n",
    "    cloudMask = qa.bitwiseAnd(cloudBitMask).eq(0).And(qa.bitwiseAnd(cirrusBitMask).eq(0))\n",
    "\n",
    "    # Apply the cloud mask to the image\n",
    "    maskedImage = image.updateMask(cloudMask)\n",
    "\n",
    "    return maskedImage\n",
    "\n",
    "\n",
    "# Define the function to compute the mean reflectance values for the specified bands within the region of interest (lake)\n",
    "def reflectance(img, lake):\n",
    "    reflectance_values = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=lake, scale=30).select(STD_NEW)\n",
    "    return img.set('DATE_SMP', img.date().format()).set('reflectance', reflectance_values)\n",
    "\n",
    "# Initialize an empty list to store the dataframes for each lake\n",
    "dfs = []\n",
    "\n",
    "# Create a dictionary to map each lake name to a numerical label\n",
    "lake_name_to_label = {lake_name: i for i, lake_name in enumerate(lake_names)}\n",
    "\n",
    "# Loop through each lake name and retrieve Landsat 5 imagery for that lake\n",
    "for SITE_NAME in lake_names:\n",
    "    # Retrieve Landsat 5 imagery for the specific lake\n",
    "    lake = ee.FeatureCollection('projects/ee-touhedakhanom14/assets/stations-coord')\\\n",
    "        .filter(ee.Filter.eq('SITE_NAME', SITE_NAME))\n",
    "    \n",
    "    s2_new = ee.ImageCollection('COPERNICUS/S2_SR') \\\n",
    "        .filter(ee.Filter.calendarRange(2019, 2023, 'year')) \\\n",
    "        .filterBounds(lake) \\\n",
    "        .filter(ee.Filter.lt('CLOUD_COVER', 15)) \\\n",
    "        .map(maskS2clouds) \\\n",
    "        .map(normalizedBands)\\\n",
    "        .select(new_bands,STD_NEW)\n",
    "    \n",
    "   # Map.addLayer(lake, {'color': 'red'})  # Adding the original point to show the difference\n",
    "    \n",
    "#     s2 = ee.ImageCollection('COPERNICUS/S2_SR') \\\n",
    "#     .filter(ee.Filter.calendarRange(2019, 2023, 'year')) \\\n",
    "#     .map(maskS2clouds) \\\n",
    "#     .select(sentinel2_bands, STD_NAMES)\\\n",
    "#     .filterBounds(lake) \\\n",
    "#     .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 50))\n",
    "\n",
    "    # Map the reflectance function over the Landsat 5 ImageCollection for the specific lake\n",
    "    map_reflectance = s2_new.map(lambda img: reflectance(img, lake))\n",
    "\n",
    "    # Reduce the mapped image collection to get reflectance values for the specific lake\n",
    "    list_reflectance = map_reflectance.reduceColumns(ee.Reducer.toList(2), ['DATE_SMP', 'reflectance']).values().get(0)\n",
    "\n",
    "    # Convert the results to a pandas DataFrame\n",
    "    df_reflectance = pd.DataFrame(list_reflectance.getInfo(), columns=['DATE_SMP', 'reflectance'])\n",
    "    df_reflectance['DATE_SMP'] = pd.to_datetime(df_reflectance['DATE_SMP'])\n",
    "    df_reflectance['DATE_SMP'] = df_reflectance['DATE_SMP'].dt.date\n",
    "    df_reflectance['reflectance'] = df_reflectance['reflectance'].apply(lambda x: {k: v/10000 for k, v in x.items() if v is not None})\n",
    "\n",
    "    # Unpack the 'reflectance' dictionary and create separate columns for each band\n",
    "    df_reflectance = pd.concat([df_reflectance.drop('reflectance', axis=1),\n",
    "                                df_reflectance['reflectance'].apply(pd.Series).astype('float64', errors='ignore')], axis=1)\n",
    "\n",
    "    # Add a new column for the lake name\n",
    "    df_reflectance['SITE_NAME'] = SITE_NAME\n",
    "    \n",
    "    # Add a new column for the lake label (numeric representation)\n",
    "    df_reflectance['LAKE_LABEL'] = lake_name_to_label[SITE_NAME]\n",
    "\n",
    "    # Add the DataFrame to the list\n",
    "    dfs.append(df_reflectance)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "df_all_lakes = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Sort the DataFrame by 'DATE_SMP' in ascending order\n",
    "df_all_lakes.sort_values(by='DATE_SMP', inplace=True)\n",
    "\n",
    "\n",
    "# Map.centerObject(lake, 10)\n",
    "# Map\n",
    "# Display the DataFrame containing reflectance values for all lakes with separate columns for each lake\n",
    "# print(df_all_lakes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7301b39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE_SMP</th>\n",
       "      <th>reflectance</th>\n",
       "      <th>SITE_NAME</th>\n",
       "      <th>LAKE_LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [DATE_SMP, reflectance, SITE_NAME, LAKE_LABEL]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcc2e9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_lakes.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d601fe4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE_SMP</th>\n",
       "      <th>reflectance</th>\n",
       "      <th>SITE_NAME</th>\n",
       "      <th>LAKE_LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [DATE_SMP, reflectance, SITE_NAME, LAKE_LABEL]\n",
       "Index: []"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4e09235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [DATE_SMP, reflectance, SITE_NAME, LAKE_LABEL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "print(df_all_lakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3c2946c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_lakes['DATE_SMP'] = pd.to_datetime(df_all_lakes['DATE_SMP'])\n",
    "data_modify['DATE_SMP'] = pd.to_datetime(data_modify['DATE_SMP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04d00657",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_modify.sort_values('DATE_SMP', inplace=True)\n",
    "df_all_lakes.sort_values('DATE_SMP', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a065e4cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "['Blue']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Merge data within the 3-day time window\u001b[39;00m\n\u001b[1;32m      5\u001b[0m merged_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge_asof(data_modify, df_all_lakes, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDATE_SMP\u001b[39m\u001b[38;5;124m'\u001b[39m, by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSITE_NAME\u001b[39m\u001b[38;5;124m'\u001b[39m, tolerance\u001b[38;5;241m=\u001b[39mwindow_size)\n\u001b[0;32m----> 6\u001b[0m merged_data \u001b[38;5;241m=\u001b[39m \u001b[43mmerged_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBlue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m merged_data \u001b[38;5;241m=\u001b[39m merged_data\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLAKE_DEPTH_MEAN\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      8\u001b[0m merged_data\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py:6561\u001b[0m, in \u001b[0;36mDataFrame.dropna\u001b[0;34m(self, axis, how, thresh, subset, inplace)\u001b[0m\n\u001b[1;32m   6559\u001b[0m     check \u001b[38;5;241m=\u001b[39m indices \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   6560\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m-> 6561\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(np\u001b[38;5;241m.\u001b[39marray(subset)[check]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m   6562\u001b[0m     agg_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indices, axis\u001b[38;5;241m=\u001b[39magg_axis)\n\u001b[1;32m   6564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m thresh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_default:\n",
      "\u001b[0;31mKeyError\u001b[0m: ['Blue']"
     ]
    }
   ],
   "source": [
    "# Create the 3-day time window\n",
    "window_size = pd.Timedelta(days=3)\n",
    "\n",
    "# Merge data within the 3-day time window\n",
    "merged_data = pd.merge_asof(data_modify, df_all_lakes, on='DATE_SMP', by='SITE_NAME', tolerance=window_size)\n",
    "merged_data = merged_data.dropna(subset=['Blue'])\n",
    "merged_data = merged_data.dropna(subset=['LAKE_DEPTH_MEAN'])\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656f9c1e",
   "metadata": {},
   "source": [
    "# Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff47a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_rows, num_cols = X.shape\n",
    "# print(\"Number of rows:\", num_rows)\n",
    "# print(\"Number of columns:\", num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94369f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea9ea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1c1fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preparing the training data\n",
    "# X = merged_data[['LAKE_LABEL','Aerosols', 'Blue', 'Green', 'Red', 'RedEdge1','RedEdge2','Cloud','NIR','SWIR1','LAKE_DEPTH_MEAN']]  # Features\n",
    "# y = merged_data['DOC_MG_L']  # Target variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4092ab49",
   "metadata": {},
   "source": [
    "# WITHOUT LAKE LABEL AS FEATURES 3 day window/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6af6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "# from lightgbm import LGBMRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Preparing the training data\n",
    "X = merged_data[['Blue', 'Green', 'Red','LAKE_DEPTH_MEAN']]  # Features\n",
    "y = merged_data['DOC_MG_L']  # Target variable\n",
    "\n",
    "# Group the data by 'LAKE_LABEL'\n",
    "grouped_data = merged_data.groupby('LAKE_LABEL')\n",
    "\n",
    "# Initialize lists to store the training and testing data for all lakes\n",
    "X_train_all, X_test_all, y_train_all, y_test_all = [], [], [], []\n",
    "\n",
    "# Loop through each lake group and split data into training and testing sets\n",
    "for _, lake_group in grouped_data:\n",
    "    if len(lake_group) < 2:\n",
    "        # Skip this lake group as it has insufficient data for splitting\n",
    "        continue\n",
    "    X_lake = lake_group[['Blue', 'Green', 'Red','LAKE_DEPTH_MEAN']]\n",
    "    y_lake = lake_group['DOC_MG_L']\n",
    "   \n",
    "    # Split data into training and testing sets for the current lake\n",
    "    X_train_lake, X_test_lake, y_train_lake, y_test_lake = train_test_split(X_lake, y_lake, test_size=0.3, random_state=42)\n",
    "   \n",
    "    # Append the data for the current lake to the corresponding lists\n",
    "    X_train_all.append(X_train_lake)\n",
    "    X_test_all.append(X_test_lake)\n",
    "    y_train_all.append(y_train_lake)\n",
    "    y_test_all.append(y_test_lake)\n",
    "\n",
    "# Concatenate the training and testing data for all lakes\n",
    "X_train = pd.concat(X_train_all)\n",
    "X_test = pd.concat(X_test_all)\n",
    "y_train = pd.concat(y_train_all)\n",
    "y_test = pd.concat(y_test_all)\n",
    "\n",
    "print('**************Data Points Used***************************')\n",
    "num_rows, num_cols = X.shape\n",
    "print(\"Number of rows:\", num_rows)\n",
    "print(\"Number of columns:\", num_cols)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'RandomForest': RandomForestRegressor(random_state=42),\n",
    "    'AdaBoost': AdaBoostRegressor(random_state=42),\n",
    "    'XGBoost': XGBRegressor(random_state=42),\n",
    "    'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
    "    'SVR': SVR(),\n",
    "    'MLPRegressor': MLPRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Initialize lists to store model evaluation results\n",
    "model_names, r2_scores, mses, rmses, maes = [], [], [], [], []\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict DOC values for the testing data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Create DataFrames for the training and testing data with all bands and the predicted values\n",
    "    data_train = X_train.copy()\n",
    "    data_train['original_DOC'] = y_train\n",
    "    data_train['predicted_DOC'] = model.predict(X_train)\n",
    "\n",
    "    data_test = X_test.copy()\n",
    "    data_test['original_DOC'] = y_test\n",
    "    data_test['predicted_DOC'] = y_pred\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    # Append the results to the lists\n",
    "    model_names.append(model_name)\n",
    "    r2_scores.append(r2)\n",
    "    mses.append(mse)\n",
    "    rmses.append(rmse)\n",
    "    maes.append(mae)\n",
    "\n",
    "# Display the predicted DOC values for the training and testing data\n",
    "print(f'*************Predicted Training data ({model_name})***************')\n",
    "print(data_train)\n",
    "\n",
    "print(f'*************Predicted Testing data ({model_name})***************')\n",
    "print(data_test)\n",
    "\n",
    "# Create a dataframe to store the results\n",
    "evaluation_df_WITHOUT3 = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'R^2': r2_scores,\n",
    "    'MSE': mses,\n",
    "    'RMSE': rmses,\n",
    "    'MAE': maes\n",
    "})\n",
    "\n",
    "# Print the dataframe\n",
    "print('# WITHOUT LAKE LABEL AS FEATURES 3 day window/n')\n",
    "print(evaluation_df_WITHOUT3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d80f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation_df_WITHOUT3.to_excel(r'/Users/touhedakhanom/Downloads/NSF SUMMER Research /Research /Machine Learning/ML/EXCEL SURFACE REFLECTANCE\\SR WITHOUT LAKE LABEL AS FEATURES 3 day window.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58708c7d",
   "metadata": {},
   "source": [
    "# WITH LAKE FEATURE AS 3 DAY WINDOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a716fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Preparing the training data\n",
    "X = merged_data[['LAKE_LABEL','Blue', 'Green', 'Red','LAKE_DEPTH_MEAN']]  # Features\n",
    "y = merged_data['DOC_MG_L']  # Target variable\n",
    "\n",
    "# Group the data by 'LAKE_LABEL'\n",
    "grouped_data = merged_data.groupby('LAKE_LABEL')\n",
    "\n",
    "# Initialize lists to store the training and testing data for all lakes\n",
    "X_train_all, X_test_all, y_train_all, y_test_all = [], [], [], []\n",
    "\n",
    "# Loop through each lake group and split data into training and testing sets\n",
    "for _, lake_group in grouped_data:\n",
    "    if len(lake_group) < 2:\n",
    "        # Skip this lake group as it has insufficient data for splitting\n",
    "        continue\n",
    "    X_lake = lake_group[['LAKE_LABEL','Blue', 'Green', 'Red', 'LAKE_DEPTH_MEAN']]\n",
    "    y_lake = lake_group['DOC_MG_L']\n",
    "   \n",
    "    # Split data into training and testing sets for the current lake\n",
    "    X_train_lake, X_test_lake, y_train_lake, y_test_lake = train_test_split(X_lake, y_lake, test_size=0.3, random_state=42)\n",
    "   \n",
    "    # Append the data for the current lake to the corresponding lists\n",
    "    X_train_all.append(X_train_lake)\n",
    "    X_test_all.append(X_test_lake)\n",
    "    y_train_all.append(y_train_lake)\n",
    "    y_test_all.append(y_test_lake)\n",
    "\n",
    "# Concatenate the training and testing data for all lakes\n",
    "X_train = pd.concat(X_train_all)\n",
    "X_test = pd.concat(X_test_all)\n",
    "y_train = pd.concat(y_train_all)\n",
    "y_test = pd.concat(y_test_all)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('**************Data Points Used***************************')\n",
    "num_rows, num_cols = X.shape\n",
    "print(\"Number of rows:\", num_rows)\n",
    "print(\"Number of columns:\", num_cols)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'RandomForest': RandomForestRegressor(random_state=42),\n",
    "    'AdaBoost': AdaBoostRegressor(random_state=42),\n",
    "    'XGBoost': XGBRegressor(random_state=42),\n",
    "    'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
    "    'SVR': SVR(),\n",
    "    'MLPRegressor': MLPRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Initialize lists to store model evaluation results\n",
    "model_names, r2_scores, mses, rmses, maes = [], [], [], [], []\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict DOC values for the testing data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Create DataFrames for the training and testing data with all bands and the predicted values\n",
    "    data_train = X_train.copy()\n",
    "    data_train['original_DOC'] = y_train\n",
    "    data_train['predicted_DOC'] = model.predict(X_train)\n",
    "\n",
    "    data_test = X_test.copy()\n",
    "    data_test['original_DOC'] = y_test\n",
    "    data_test['predicted_DOC'] = y_pred\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    # Append the results to the lists\n",
    "    model_names.append(model_name)\n",
    "    r2_scores.append(r2)\n",
    "    mses.append(mse)\n",
    "    rmses.append(rmse)\n",
    "    maes.append(mae)\n",
    "\n",
    "# Display the predicted DOC values for the training and testing data\n",
    "print(f'*************Predicted Training data ({model_name})***************')\n",
    "print(data_train)\n",
    "\n",
    "print(f'*************Predicted Testing data ({model_name})***************')\n",
    "print(data_test)\n",
    "\n",
    "# Create a dataframe to store the results\n",
    "evaluation_df_WITH3 = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'R^2': r2_scores,\n",
    "    'MSE': mses,\n",
    "    'RMSE': rmses,\n",
    "    'MAE': maes\n",
    "})\n",
    "\n",
    "# Print the dataframe\n",
    "print('# WITH LAKE LABEL AS FEATURES 3 day window/n')\n",
    "print(evaluation_df_WITH3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5592f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scatter plot for all lakes combined\n",
    "plt.scatter( data_train['original_DOC'],data_train['predicted_DOC'], color='blue', label='Training Data')\n",
    "plt.scatter(data_test['original_DOC'],data_test['predicted_DOC'],  color='red', label='Testing Data')\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title('Scatter Plot for All Lakes')\n",
    "plt.xlabel('Original DOC')\n",
    "plt.ylabel('Predicted DOC')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234b2590",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation_df_WITH3.to_excel(r'/Users/touhedakhanom/Downloads/NSF SUMMER Research /Research /Machine Learning/ML/EXCEL SURFACE REFLECTANCE\\SR WITH LAKE LABEL AS FEATURES 3 day window.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9156b82f",
   "metadata": {},
   "source": [
    "# WITHOUT LAKE LABEL AS FEATURES 1 day window/n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf3b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 3-day time window\n",
    "window_size = pd.Timedelta(days=1)\n",
    "\n",
    "# Merge data within the 3-day time window\n",
    "merged_data_1 = pd.merge_asof(data_modify, df_all_lakes, on='DATE_SMP', by='SITE_NAME', tolerance=window_size)\n",
    "merged_data_1 = merged_data_1.dropna(subset=['Blue'])\n",
    "merged_data_1 = merged_data_1.dropna(subset=['LAKE_DEPTH_MEAN'])\n",
    "merged_data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf7e75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "# from lightgbm import LGBMRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Preparing the training data\n",
    "X = merged_data_1[['Blue', 'Green', 'Red','LAKE_DEPTH_MEAN']]  # Features\n",
    "y = merged_data_1['DOC_MG_L']  # Target variable\n",
    "\n",
    "# Group the data by 'LAKE_LABEL'\n",
    "grouped_data = merged_data_1.groupby('LAKE_LABEL')\n",
    "\n",
    "# Initialize lists to store the training and testing data for all lakes\n",
    "X_train_all, X_test_all, y_train_all, y_test_all = [], [], [], []\n",
    "\n",
    "# Loop through each lake group and split data into training and testing sets\n",
    "for _, lake_group in grouped_data:\n",
    "    if len(lake_group) < 4: #Give me 0.3 but this is better \n",
    "        # Skip this lake group as it has insufficient data for splitting\n",
    "        continue\n",
    "    X_lake = lake_group[['Blue', 'Green', 'Red','LAKE_DEPTH_MEAN']]\n",
    "    y_lake = lake_group['DOC_MG_L']\n",
    "   \n",
    "    # Split data into training and testing sets for the current lake\n",
    "    X_train_lake, X_test_lake, y_train_lake, y_test_lake = train_test_split(X_lake, y_lake, test_size=0.3, random_state=42)\n",
    "   \n",
    "    # Append the data for the current lake to the corresponding lists\n",
    "    X_train_all.append(X_train_lake)\n",
    "    X_test_all.append(X_test_lake)\n",
    "    y_train_all.append(y_train_lake)\n",
    "    y_test_all.append(y_test_lake)\n",
    "\n",
    "# Concatenate the training and testing data for all lakes\n",
    "X_train = pd.concat(X_train_all)\n",
    "X_test = pd.concat(X_test_all)\n",
    "y_train = pd.concat(y_train_all)\n",
    "y_test = pd.concat(y_test_all)\n",
    "\n",
    "print('**************Data Points Used***************************')\n",
    "num_rows, num_cols = X.shape\n",
    "print(\"Number of rows:\", num_rows)\n",
    "print(\"Number of columns:\", num_cols)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'RandomForest': RandomForestRegressor(random_state=42),\n",
    "    'AdaBoost': AdaBoostRegressor(random_state=42),\n",
    "    'XGBoost': XGBRegressor(random_state=42),\n",
    "    'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
    "    'SVR': SVR(),\n",
    "    'MLPRegressor': MLPRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Initialize lists to store model evaluation results\n",
    "model_names, r2_scores, mses, rmses, maes = [], [], [], [], []\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict DOC values for the testing data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Create DataFrames for the training and testing data with all bands and the predicted values\n",
    "    data_train = X_train.copy()\n",
    "    data_train['original_DOC'] = y_train\n",
    "    data_train['predicted_DOC'] = model.predict(X_train)\n",
    "\n",
    "    data_test = X_test.copy()\n",
    "    data_test['original_DOC'] = y_test\n",
    "    data_test['predicted_DOC'] = y_pred\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    # Append the results to the lists\n",
    "    model_names.append(model_name)\n",
    "    r2_scores.append(r2)\n",
    "    mses.append(mse)\n",
    "    rmses.append(rmse)\n",
    "    maes.append(mae)\n",
    "\n",
    "# Display the predicted DOC values for the training and testing data\n",
    "print(f'*************Predicted Training data ({model_name})***************')\n",
    "print(data_train)\n",
    "\n",
    "print(f'*************Predicted Testing data ({model_name})***************')\n",
    "print(data_test)\n",
    "\n",
    "# Create a dataframe to store the results\n",
    "evaluation_df_WITHOUT1 = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'R^2': r2_scores,\n",
    "    'MSE': mses,\n",
    "    'RMSE': rmses,\n",
    "    'MAE': maes\n",
    "})\n",
    "\n",
    "# Print the dataframe\n",
    "print('# WITHOUT LAKE LABEL AS FEATURES 1 day window/n')\n",
    "print(evaluation_df_WITHOUT1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f07eb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation_df_WITHOUT1.to_excel(r'/Users/touhedakhanom/Downloads/NSF SUMMER Research /Research /Machine Learning/ML/EXCEL SURFACE REFLECTANCE\\SR WITHOUT LAKE LABEL AS FEATURES 1 day window.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77055fc1",
   "metadata": {},
   "source": [
    "# WITH LAKE LABEL AS FEATURES 1 day window/n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c751a12f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Preparing the training data\n",
    "X = merged_data_1[['LAKE_LABEL','Blue', 'Green', 'Red','LAKE_DEPTH_MEAN']]  # Features\n",
    "y = merged_data_1['DOC_MG_L']  # Target variable\n",
    "\n",
    "# Group the data by 'LAKE_LABEL'\n",
    "grouped_data = merged_data_1.groupby('LAKE_LABEL')\n",
    "\n",
    "# Initialize lists to store the training and testing data for all lakes\n",
    "X_train_all, X_test_all, y_train_all, y_test_all = [], [], [], []\n",
    "\n",
    "# Loop through each lake group and split data into training and testing sets\n",
    "for _, lake_group in grouped_data:\n",
    "    if len(lake_group) < 4:\n",
    "        # Skip this lake group as it has insufficient data for splitting\n",
    "        continue\n",
    "    X_lake = lake_group[['LAKE_LABEL','Blue', 'Green', 'Red', 'LAKE_DEPTH_MEAN']]\n",
    "    y_lake = lake_group['DOC_MG_L']\n",
    "   \n",
    "    # Split data into training and testing sets for the current lake\n",
    "    X_train_lake, X_test_lake, y_train_lake, y_test_lake = train_test_split(X_lake, y_lake, test_size=0.3, random_state=42)\n",
    "   \n",
    "    # Append the data for the current lake to the corresponding lists\n",
    "    X_train_all.append(X_train_lake)\n",
    "    X_test_all.append(X_test_lake)\n",
    "    y_train_all.append(y_train_lake)\n",
    "    y_test_all.append(y_test_lake)\n",
    "\n",
    "# Concatenate the training and testing data for all lakes\n",
    "X_train = pd.concat(X_train_all)\n",
    "X_test = pd.concat(X_test_all)\n",
    "y_train = pd.concat(y_train_all)\n",
    "y_test = pd.concat(y_test_all)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('**************Data Points Used***************************')\n",
    "num_rows, num_cols = X.shape\n",
    "print(\"Number of rows:\", num_rows)\n",
    "print(\"Number of columns:\", num_cols)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'RandomForest': RandomForestRegressor(random_state=42),\n",
    "    'AdaBoost': AdaBoostRegressor(random_state=42),\n",
    "    'XGBoost': XGBRegressor(random_state=42),\n",
    "    'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
    "    'SVR': SVR(),\n",
    "    'MLPRegressor': MLPRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Initialize lists to store model evaluation results\n",
    "model_names, r2_scores, mses, rmses, maes = [], [], [], [], []\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict DOC values for the testing data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Create DataFrames for the training and testing data with all bands and the predicted values\n",
    "    data_train = X_train.copy()\n",
    "    data_train['original_DOC'] = y_train\n",
    "    data_train['predicted_DOC'] = model.predict(X_train)\n",
    "\n",
    "    data_test = X_test.copy()\n",
    "    data_test['original_DOC'] = y_test\n",
    "    data_test['predicted_DOC'] = y_pred\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    # Append the results to the lists\n",
    "    model_names.append(model_name)\n",
    "    r2_scores.append(r2)\n",
    "    mses.append(mse)\n",
    "    rmses.append(rmse)\n",
    "    maes.append(mae)\n",
    "\n",
    "# Display the predicted DOC values for the training and testing data\n",
    "print(f'*************Predicted Training data ({model_name})***************')\n",
    "print(data_train)\n",
    "\n",
    "print(f'*************Predicted Testing data ({model_name})***************')\n",
    "print(data_test)\n",
    "\n",
    "# Create a dataframe to store the results\n",
    "evaluation_df_WITH1 = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'R^2': r2_scores,\n",
    "    'MSE': mses,\n",
    "    'RMSE': rmses,\n",
    "    'MAE': maes\n",
    "})\n",
    "\n",
    "# Print the dataframe\n",
    "print('# WITH LAKE LABEL AS FEATURES 1 day window/n')\n",
    "print(evaluation_df_WITH1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d10a49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation_df_WITH1.to_excel(r'/Users/touhedakhanom/Downloads/NSF SUMMER Research /Research /Machine Learning/ML/EXCEL SURFACE REFLECTANCE\\SR WITH LAKE LABEL AS FEATURES 1 day window.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1de6928",
   "metadata": {},
   "source": [
    "# WITHOUT LAKE LABEL AS FEATURES 7 day window/n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a17d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 3-day time window\n",
    "window_size = pd.Timedelta(days=7)\n",
    "\n",
    "# Merge data within the 3-day time window\n",
    "merged_data_7 = pd.merge_asof(data_modify, df_all_lakes, on='DATE_SMP', by='SITE_NAME', tolerance=window_size)\n",
    "merged_data_7 = merged_data_7.dropna(subset=['Blue'])\n",
    "merged_data_7 = merged_data_7.dropna(subset=['LAKE_DEPTH_MEAN'])\n",
    "merged_data_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ebd8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "# from lightgbm import LGBMRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Preparing the training data\n",
    "X = merged_data_7[['Blue', 'Green', 'Red','LAKE_DEPTH_MEAN']]  # Features\n",
    "y = merged_data_7['DOC_MG_L']  # Target variable\n",
    "\n",
    "# Group the data by 'LAKE_LABEL'\n",
    "grouped_data = merged_data_7.groupby('LAKE_LABEL')\n",
    "\n",
    "# Initialize lists to store the training and testing data for all lakes\n",
    "X_train_all, X_test_all, y_train_all, y_test_all = [], [], [], []\n",
    "\n",
    "# Loop through each lake group and split data into training and testing sets\n",
    "for _, lake_group in grouped_data:\n",
    "    if len(lake_group) < 4:\n",
    "        # Skip this lake group as it has insufficient data for splitting\n",
    "        continue\n",
    "    X_lake = lake_group[['Blue', 'Green', 'Red','LAKE_DEPTH_MEAN']]\n",
    "    y_lake = lake_group['DOC_MG_L']\n",
    "   \n",
    "    # Split data into training and testing sets for the current lake\n",
    "    X_train_lake, X_test_lake, y_train_lake, y_test_lake = train_test_split(X_lake, y_lake, test_size=0.3, random_state=42)\n",
    "   \n",
    "    # Append the data for the current lake to the corresponding lists\n",
    "    X_train_all.append(X_train_lake)\n",
    "    X_test_all.append(X_test_lake)\n",
    "    y_train_all.append(y_train_lake)\n",
    "    y_test_all.append(y_test_lake)\n",
    "\n",
    "# Concatenate the training and testing data for all lakes\n",
    "X_train = pd.concat(X_train_all)\n",
    "X_test = pd.concat(X_test_all)\n",
    "y_train = pd.concat(y_train_all)\n",
    "y_test = pd.concat(y_test_all)\n",
    "\n",
    "print('**************Data Points Used***************************')\n",
    "num_rows, num_cols = X.shape\n",
    "print(\"Number of rows:\", num_rows)\n",
    "print(\"Number of columns:\", num_cols)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'RandomForest': RandomForestRegressor(random_state=42),\n",
    "    'AdaBoost': AdaBoostRegressor(random_state=42),\n",
    "    'XGBoost': XGBRegressor(random_state=42),\n",
    "    'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
    "    'SVR': SVR(),\n",
    "    'MLPRegressor': MLPRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Initialize lists to store model evaluation results\n",
    "model_names, r2_scores, mses, rmses, maes = [], [], [], [], []\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict DOC values for the testing data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Create DataFrames for the training and testing data with all bands and the predicted values\n",
    "    data_train = X_train.copy()\n",
    "    data_train['original_DOC'] = y_train\n",
    "    data_train['predicted_DOC'] = model.predict(X_train)\n",
    "\n",
    "    data_test = X_test.copy()\n",
    "    data_test['original_DOC'] = y_test\n",
    "    data_test['predicted_DOC'] = y_pred\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    # Append the results to the lists\n",
    "    model_names.append(model_name)\n",
    "    r2_scores.append(r2)\n",
    "    mses.append(mse)\n",
    "    rmses.append(rmse)\n",
    "    maes.append(mae)\n",
    "\n",
    "# Display the predicted DOC values for the training and testing data\n",
    "print(f'*************Predicted Training data ({model_name})***************')\n",
    "print(data_train)\n",
    "\n",
    "print(f'*************Predicted Testing data ({model_name})***************')\n",
    "print(data_test)\n",
    "\n",
    "# Create a dataframe to store the results\n",
    "evaluation_df_WITHOUT7 = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'R^2': r2_scores,\n",
    "    'MSE': mses,\n",
    "    'RMSE': rmses,\n",
    "    'MAE': maes\n",
    "})\n",
    "\n",
    "# Print the dataframe\n",
    "print('# WITHOUT LAKE LABEL AS FEATURES 7 day window/n')\n",
    "print(evaluation_df_WITHOUT7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5272ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df_WITHOUT7.to_excel(r'/Users/touhedakhanom/Downloads/NSF SUMMER Research /Research /Machine Learning/ML/EXCEL SURFACE REFLECTANCE\\SR WITHOUT LAKE LABEL AS FEATURES 7 day window.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb200c29",
   "metadata": {},
   "source": [
    "# WITH LAKE LABEL AS FEATURES 7 day window/n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b5829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Preparing the training data\n",
    "X = merged_data_7[['LAKE_LABEL', 'Aerosols', 'Blue', 'Green', 'Red', 'RedEdge1', 'RedEdge2', 'Cloud', 'NIR', 'SWIR1', 'LAKE_DEPTH_MEAN']]  # Features\n",
    "y = merged_data_7['DOC_MG_L']  # Target variable\n",
    "\n",
    "# Group the data by 'LAKE_LABEL'\n",
    "grouped_data = merged_data_7.groupby('LAKE_LABEL')\n",
    "\n",
    "# Initialize lists to store the training and testing data for all lakes\n",
    "X_train_all, X_test_all, y_train_all, y_test_all = [], [], [], []\n",
    "\n",
    "# Loop through each lake group and split data into training and testing sets\n",
    "for _, lake_group in grouped_data:\n",
    "    if len(lake_group) < 4:\n",
    "        # Skip this lake group as it has insufficient data for splitting\n",
    "        continue\n",
    "    X_lake = lake_group[['LAKE_LABEL', 'Aerosols', 'Blue', 'Green', 'Red', 'RedEdge1', 'RedEdge2', 'Cloud', 'NIR', 'SWIR1', 'LAKE_DEPTH_MEAN']]\n",
    "    y_lake = lake_group['DOC_MG_L']\n",
    "   \n",
    "    # Split data into training and testing sets for the current lake\n",
    "    X_train_lake, X_test_lake, y_train_lake, y_test_lake = train_test_split(X_lake, y_lake, test_size=0.3, random_state=42)\n",
    "   \n",
    "    # Append the data for the current lake to the corresponding lists\n",
    "    X_train_all.append(X_train_lake)\n",
    "    X_test_all.append(X_test_lake)\n",
    "    y_train_all.append(y_train_lake)\n",
    "    y_test_all.append(y_test_lake)\n",
    "\n",
    "# Concatenate the training and testing data for all lakes\n",
    "X_train = pd.concat(X_train_all)\n",
    "X_test = pd.concat(X_test_all)\n",
    "y_train = pd.concat(y_train_all)\n",
    "y_test = pd.concat(y_test_all)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('**************Data Points Used***************************')\n",
    "num_rows, num_cols = X.shape\n",
    "print(\"Number of rows:\", num_rows)\n",
    "print(\"Number of columns:\", num_cols)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'RandomForest': RandomForestRegressor(random_state=42),\n",
    "    'AdaBoost': AdaBoostRegressor(random_state=42),\n",
    "    'XGBoost': XGBRegressor(random_state=42),\n",
    "    'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
    "    'SVR': SVR(),\n",
    "    'MLPRegressor': MLPRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Initialize lists to store model evaluation results\n",
    "model_names, r2_scores, mses, rmses, maes = [], [], [], [], []\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict DOC values for the testing data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Create DataFrames for the training and testing data with all bands and the predicted values\n",
    "    data_train = X_train.copy()\n",
    "    data_train['original_DOC'] = y_train\n",
    "    data_train['predicted_DOC'] = model.predict(X_train)\n",
    "\n",
    "    data_test = X_test.copy()\n",
    "    data_test['original_DOC'] = y_test\n",
    "    data_test['predicted_DOC'] = y_pred\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    # Append the results to the lists\n",
    "    model_names.append(model_name)\n",
    "    r2_scores.append(r2)\n",
    "    mses.append(mse)\n",
    "    rmses.append(rmse)\n",
    "    maes.append(mae)\n",
    "\n",
    "# Display the predicted DOC values for the training and testing data\n",
    "print(f'*************Predicted Training data ({model_name})***************')\n",
    "print(data_train)\n",
    "\n",
    "print(f'*************Predicted Testing data ({model_name})***************')\n",
    "print(data_test)\n",
    "\n",
    "# Create a dataframe to store the results\n",
    "evaluation_df_WITH7 = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'R^2': r2_scores,\n",
    "    'MSE': mses,\n",
    "    'RMSE': rmses,\n",
    "    'MAE': maes\n",
    "})\n",
    "\n",
    "# Print the dataframe\n",
    "print('# WITH LAKE LABEL AS FEATURES 7 day window/n')\n",
    "print(evaluation_df_WITH7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08722ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation_df_WITH7.to_excel(r'/Users/touhedakhanom/Downloads/NSF SUMMER Research /Research /Machine Learning/ML\\SR WITH LAKE LABEL AS FEATURES 7 day window.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50859fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scatter plot for all lakes combined\n",
    "plt.scatter( data_train['original_DOC'],data_train['predicted_DOC'], color='blue', label='Training Data')\n",
    "plt.scatter(data_test['original_DOC'],data_test['predicted_DOC'],  color='red', label='Testing Data')\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title('Scatter Plot for All Lakes')\n",
    "plt.xlabel('Original DOC')\n",
    "plt.ylabel('Predicted DOC')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d993a64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
